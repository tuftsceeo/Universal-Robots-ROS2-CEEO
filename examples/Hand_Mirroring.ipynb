{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee76e810-a5f2-4856-8320-1e8798fd0c53",
   "metadata": {},
   "source": [
    "# Hand Mirroring\n",
    "This notebook demonstrates how you can get the robot arm to mirror your hand using a webcam. You will need a laptop, a working MQTT broker, a running UR driver, a running MQTT forwarder, and an instance of the MyUR3e and Sub_Node class.\n",
    "\n",
    "## On your Laptop:\n",
    "Install opencv and paho-mqtt using pip. Then run this code locally on your laptop and find a blue object to hold in your hand for the camera to track. You can also change the color thresholds to track a different color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1075e39-c32b-4248-8205-488ed825affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import paho.mqtt.client as mqtt\n",
    "import json\n",
    "\n",
    "# MQTT setup\n",
    "broker_address = \"130.64.16.222\"  # Replace with your MQTT broker address\n",
    "broker_port = 1884 # Replace with your MQTT broker port\n",
    "client = mqtt.Client(\"opencv_tracker\") # Name of client\n",
    "client.connect(broker_address,broker_port)\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the range of color to track\n",
    "# In this instance we track a range of blue in HSV (Hue, Saturation, Value)\n",
    "lower_blue = np.array([90, 50, 70])\n",
    "upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Mirror the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for the blue color\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If contours are found, track the largest one\n",
    "    if contours:\n",
    "        # Find the largest contour\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        # Draw the bounding box around the blue object\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Calculate the center of the bounding box from 0-1\n",
    "        center_x = (x + w // 2) / frame_width\n",
    "        center_y = (y + h // 2) / frame_height\n",
    "\n",
    "        # Display the coordinates\n",
    "        cv2.putText(frame, f\"Coordinates: ({center_x}, {center_y})\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Send coordinates over MQTT to the \"camera\" topic\n",
    "        payload = json.dumps({\"x\": center_x, \"y\": center_y})\n",
    "        client.publish(\"camera\", payload)\n",
    "        print(\"Published: \", payload)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Disconnect MQTT client\n",
    "client.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc9287-6611-4891-95d5-a947555387a5",
   "metadata": {},
   "source": [
    "## On your Jupyter Hub:\n",
    "Make sure your UR Driver is running and that you have started an MQTT forwarder for the \"camera\" topic. Then run the code below to start controlling the UR arm based on the positions of the color tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da69d8a-16e2-4d04-bfce-37b326b66852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myur import MyUR3e\n",
    "from myur import sub_node\n",
    "\n",
    "robot = MyUR3e()\n",
    "tracker = sub_node.Sub_Node('mqtt/camera')\n",
    "\n",
    "def in_invisible_fence(pos):\n",
    "    bottom = 0.2\n",
    "    top = 0.4\n",
    "    if pos < bottom or pos > top:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    coords = json.loads(tracker.get_data())\n",
    "    y = 0.2 + (1-coords.get('y')) * 0.2 # y camera axis -> z robot axis\n",
    "    x = 0.2 + (1-coords.get('x')) * 0.2 # x camera axis -> y robot axis\n",
    "    print(\"Z: \",y)\n",
    "    if in_invisible_fence(y) and in_invisible_fence(x):\n",
    "        print(\"Moving arm\")\n",
    "        robot.move_global([0.1,x,y,180,0,0],time=('cv',0.1,0.1))\n",
    "    else:\n",
    "        print(\"Out of bounds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ros_py310]",
   "language": "python",
   "name": "conda-env-ros_py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
